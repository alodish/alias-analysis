{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ecccd2f",
   "metadata": {},
   "source": [
    "# Alias Analysis with GloVe\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8da85fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from scipy import spatial\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b838f737",
   "metadata": {},
   "source": [
    "### Attempt at utilizing [GloVe](https://nlp.stanford.edu/projects/glove/)  to associate aliases with their proper names\n",
    "Create list of names from text file filled with randomly generated names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e67eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['delmer', 'laurel', 'jody', 'lavonne', 'beth']\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "\n",
    "with open(\"names.txt\", \"r\") as txt:\n",
    "    for line in txt:\n",
    "        if line not in names:   # Avoiding duplicate occurences of names\n",
    "            names.append(line.replace(\"\\n\", \"\"))\n",
    "            \n",
    "# Now we have a list of names, but we need to make them lowercase to be usable with Glove's word vectors\n",
    "names = [i.lower() for i in names]\n",
    "\n",
    "print(names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b4b5d9",
   "metadata": {},
   "source": [
    "### Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1a1d6",
   "metadata": {},
   "source": [
    "In order to use Glove's multi-dimensional word vectors, we need to turn the vectors into dictionaries.\n",
    "\n",
    "I've already done this using the Pickle Module (.pkl files are hosted in this repository)\n",
    "\n",
    "We're going to send the list of names through each of 5 word vector dictionaries all varying in dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2390828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove-42B-300d.pkl', 'glove-6B-100d.pkl', 'glove-6B-200d.pkl', 'glove-6B-300d.pkl', 'glove-6B-50d.pkl']\n"
     ]
    }
   ],
   "source": [
    "# Get path to our pickle dictionaries and store their names in list form\n",
    "path = os.getcwd()\n",
    "dict_path = os.path.join(os.getcwd(), \"pickle-dicts/\")\n",
    "dicts = os.listdir(dict_path)\n",
    "results = [i.replace('.pkl', \".csv\") for i in dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "909bc256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for determining closely related words\n",
    "\n",
    "def find_closest_embeddings(embedding, embeddings_dict=None):\n",
    "    return sorted(embeddings_dict.keys(),\n",
    "                           key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))[1:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea2a98",
   "metadata": {},
   "source": [
    "### Now for the results loop\n",
    "We iterate through our dictionaries finding the top 5 matches for each name in our list\n",
    "\n",
    "These will be treated as key : value pairs and sent to a pandas DataFrame\n",
    "\n",
    "Each cycle of the nested loop results in the DataFrame being saved to a csv with the name of the dictionary in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1eb35853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results loop\n",
    "for d in range(len(dicts)):\n",
    "    file = open(dict_path + '/' + dicts[d], \"rb\")\n",
    "    temp_d = pickle.load(file)\n",
    "    csv_dict = {}\n",
    "    for name in names:\n",
    "        try:\n",
    "            csv_dict[name] = find_closest_embeddings(temp_d[name], embeddings_dict=temp_d)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    df = pd.DataFrame(csv_dict)\n",
    "    df.to_csv(results[d])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d5f4c",
   "metadata": {},
   "source": [
    "### Viewing the results\n",
    "With our top 5 closest matches for each dictionary now stored in CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cbd8fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove-42b-300d-results.csv \n",
      "\n",
      "beth ['becky' 'sara' 'lori' 'brenda' 'julie']\n",
      "stephen ['andrew' 'steven' 'alan' 'jonathan' 'david']\n",
      "andrew ['stephen' 'brian' 'alan' 'nicholas' 'andy']\n",
      "john ['james' 'george' 'william' 'richard' 'paul']\n",
      "jennifer ['nicole' 'jessica' 'christina' 'amanda' 'julie']\n",
      "\n",
      "\n",
      "\n",
      "glove-6b-100d-results.csv \n",
      "\n",
      "beth ['donna' 'phyllis' 'jane' 'pamela' 'joanne']\n",
      "stephen ['marshall' 'peter' 'steven' 'moore' 'andrew']\n",
      "andrew ['james' 'matthew' 'harris' 'stephen' 'stuart']\n",
      "john ['james' 'george' 'thomas' 'paul' 'william']\n",
      "jennifer ['amy' 'laura' 'michelle' 'julie' 'cynthia']\n",
      "\n",
      "\n",
      "\n",
      "glove-6B-200d-results.csv \n",
      "\n",
      "beth ['garma√Ø' 'hamedrash' 'ostrosky' 'mccarthy-miller' 'hensperger']\n",
      "stephen ['murphy' 'evans' 'miller' 'cooper' 'matthew']\n",
      "andrew ['smith' 'matthew' 'james' 'harris' 'thahl']\n",
      "john ['william' 'james' 'george' 'smith' 'thompson']\n",
      "jennifer ['connelly' 'amy' 'lisa' 'samantha' 'aniston']\n",
      "\n",
      "\n",
      "\n",
      "glove-6B-300d-results.csv \n",
      "\n",
      "beth ['marykane2000' 'svahng' 'muhlt' 'prihn' 'sihp']\n",
      "stephen ['murphy' 'bruhth' 'rohch' 'rohsh' 'steven']\n",
      "andrew ['matthew' 'thahl' 'stuart' 'bruhth' 'stephen']\n",
      "john ['james' 'rohch' 'thomas' 'hyoon' 'rohsh']\n",
      "jennifer ['amy' 'lisa' 'michelle' 'connelly' 'oxeant']\n",
      "\n",
      "\n",
      "\n",
      "glove-6B-50d-results.csv \n",
      "\n",
      "beth ['amy' 'melissa' 'tracey' 'joanne' 'walters']\n",
      "stephen ['andrew' 'matthew' 'clarke' 'peter' 'stuart']\n",
      "andrew ['stephen' 'clarke' 'stuart' 'nathan' 'howard']\n",
      "john ['james' 'william' 'thomas' 'henry' 'george']\n",
      "jennifer ['lisa' 'michelle' 'amy' 'jessica' 'lindsay']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to our results folder\n",
    "results_path = path + '/results/'\n",
    "csv_results = os.listdir(results_path)\n",
    "\n",
    "# Some of the embeddings are very lengthy so we will just take a sample of our names as key and print their values\n",
    "# This sample contains names that are either common nicknames or names with common nicknames (i.e. Andrew = Drew)\n",
    "names = ['beth', 'stephen', 'andrew', 'john', 'jennifer'] \n",
    "\n",
    "# Result viewing loop\n",
    "for csv in csv_results:\n",
    "    df = pd.DataFrame(pd.read_csv(results_path + csv))\n",
    "    print(csv, '\\n')\n",
    "    for n in names:\n",
    "        print(n, df[n].values)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a242a65",
   "metadata": {},
   "source": [
    "### Discussion of Results\n",
    "While there were some cases where nicknames were found as close embeddings, this analysis missed the mark.\n",
    "\n",
    "Only a handful of times were the expected embeddings found within the top 5 results.\n",
    "\n",
    "We did find \"Steven\" for \"Stephen\" in a few cases, but never \"Elizabeth\" for \"Beth\" or \"Jenn\" for \"Jennifer\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df613b6d",
   "metadata": {},
   "source": [
    "###  Follow-up\n",
    "Let's check if these aliases are found anywhere within the vector of embeddings\n",
    "\n",
    "This will utilize a lot of memory and is only for demonstration purposes\n",
    "\n",
    "Results are saved as .pkl file in results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8b5f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll remove the limit from our previously defined function and search for expected nicknames/ names\n",
    "\n",
    "def find_all_embeddings(embedding, embeddings_dict=None):\n",
    "    return sorted(embeddings_dict.keys(),\n",
    "                           key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))\n",
    "\n",
    "# These dictionaries found \"Steven\" for \"Stephen\"\n",
    "dicts = ['glove-42b-300d.pkl', 'glove-6B-300d.pkl', 'glove-6b-100d.pkl']\n",
    "\n",
    "# Initialize a dictionary of names that stores the relative position of expected names as values\n",
    "name_dict = {'beth' : [], 'andrew' : [], 'jennifer' : []}\n",
    "\n",
    "# List of dict keys\n",
    "names = [i for i in name_dict.keys()]\n",
    "\n",
    "# The expected values\n",
    "nicknames = ['elizabeth', 'drew', 'jenn']\n",
    "\n",
    "\n",
    "# Results loop\n",
    "for d in range(len(dicts)):\n",
    "    file = open(dict_path + '/' + dicts[d], \"rb\")\n",
    "    temp_d = pickle.load(file)\n",
    "    for n in range(len(names)):\n",
    "        temp_array = find_all_embeddings(temp_d[names[n]], embeddings_dict=temp_d)\n",
    "        try:\n",
    "            temp_idx = temp_array.index(nicknames[n])\n",
    "            temp_len = len(temp_array)\n",
    "            string = [nicknames[n] + ': found at index ' + str(temp_idx) + '/' + str(temp_len)]\n",
    "            name_dict[names[n]].extend(string)\n",
    "        except:\n",
    "            string = [nicknames[n] + ': not found in ' + str(len(temp_array)) + ' indices.']\n",
    "            name_dict[names[n]].extend(string)\n",
    "\n",
    "f = open('follow-up-results.pkl', \"wb\")\n",
    "pickle.dump(name_dict, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5bedc39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for  glove-42b-300d:  \n",
      "\n",
      "Key:  beth\n",
      "elizabeth: found at index 632/1917495 \n",
      "\n",
      "Key:  andrew\n",
      "drew: found at index 310/1917495 \n",
      "\n",
      "Key:  jennifer\n",
      "jenn: found at index 6206/1917495 \n",
      "\n",
      "\n",
      "\n",
      "Results for  glove-6B-300d:  \n",
      "\n",
      "Key:  beth\n",
      "elizabeth: found at index 272621/1958333 \n",
      "\n",
      "Key:  andrew\n",
      "drew: found at index 1035763/1958333 \n",
      "\n",
      "Key:  jennifer\n",
      "jenn: found at index 1040676/1958333 \n",
      "\n",
      "\n",
      "\n",
      "Results for  glove-6b-100d:  \n",
      "\n",
      "Key:  beth\n",
      "elizabeth: found at index 1401/400001 \n",
      "\n",
      "Key:  andrew\n",
      "drew: found at index 1327/400001 \n",
      "\n",
      "Key:  jennifer\n",
      "jenn: found at index 62073/400001 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print key:values from our results dictionary\n",
    "file = open(results_path + '/' + 'follow-up-results.pkl', \"rb\")\n",
    "temp_d = pickle.load(file)\n",
    "d_keys = [i for i in temp_d.keys()]\n",
    "\n",
    "for k in range(3):\n",
    "    print(\"Results for \", dicts[k].replace('.pkl', ': '), '\\n')\n",
    "    for i in range(3):\n",
    "        print('Key: ', d_keys[i])\n",
    "        print(temp_d[d_keys[i]][k], '\\n')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b2077",
   "metadata": {},
   "source": [
    "### Follow up discussion\n",
    "As expected, the arrays resulting from removing the limit are huge.\n",
    "\n",
    "Despite all of them containing our expected output somewhere in the array, \n",
    "\n",
    "this just isn't a feasible method for determining name-nickname associations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8437aa",
   "metadata": {},
   "source": [
    "### Verifying the results\n",
    "Based on this outcome, we are without indication that this is either an effective or efficient method of finding nicknames\n",
    "\n",
    "In a simple attempt at verifying our methods, we will take some simple nouns and check their closest embeddigs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7d0482c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for:  glove-42b-300d.pkl \n",
      "\n",
      "frog - 5 closest embeddings: \n",
      "frogs turtle monkey toad snake \n",
      "\n",
      "tree - 5 closest embeddings: \n",
      "trees branches leaf willow pine \n",
      "\n",
      "star - 5 closest embeddings: \n",
      "stars superstar winplayed musicdw moviesdw \n",
      "\n",
      "pencil - 5 closest embeddings: \n",
      "pencils pen crayon chalk drawing \n",
      "\n",
      "car - 5 closest embeddings: \n",
      "cars vehicle automobile truck auto \n",
      "\n",
      "\n",
      "\n",
      "Results for:  glove-6B-300d.pkl \n",
      "\n",
      "frog - 5 closest embeddings: \n",
      "toad frogs monkey chemicals-wholesale squirrel \n",
      "\n",
      "tree - 5 closest embeddings: \n",
      "trees pine instance yehv shade \n",
      "\n",
      "star - 5 closest embeddings: \n",
      "stars superstar http://www.nwguild.org __________________________________ once \n",
      "\n",
      "pencil - 5 closest embeddings: \n",
      "pencils crayon ink male/philippines teen.gay.ten-inch \n",
      "\n",
      "car - 5 closest embeddings: \n",
      "cars vehicle truck driver driving \n",
      "\n",
      "\n",
      "\n",
      "Results for:  glove-6b-100d.pkl \n",
      "\n",
      "frog - 5 closest embeddings: \n",
      "toad snake ape monkey frogs \n",
      "\n",
      "tree - 5 closest embeddings: \n",
      "trees grass pine bushes leaf \n",
      "\n",
      "star - 5 closest embeddings: \n",
      "stars superstar legend hero newcomer \n",
      "\n",
      "pencil - 5 closest embeddings: \n",
      "pencils crayon ink pens erasers \n",
      "\n",
      "car - 5 closest embeddings: \n",
      "vehicle truck cars driver driving \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of common nouns\n",
    "nouns = ['frog', 'tree', 'star', 'pencil', 'car']\n",
    "\n",
    "\n",
    "# Re-define our results loop to simply print results rather than save them to CSVs\n",
    "\n",
    "for d in range(len(dicts)):\n",
    "    file = open(dict_path + '/' + dicts[d], \"rb\")\n",
    "    temp_d = pickle.load(file)\n",
    "    print(\"Results for: \", dicts[d], '\\n')\n",
    "    for noun in nouns:\n",
    "        r = find_closest_embeddings(temp_d[noun], embeddings_dict=temp_d)\n",
    "        string = noun + ' - 5 closest embeddings: '\n",
    "        print(string)\n",
    "        print(*r, '\\n')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c150f3ae",
   "metadata": {},
   "source": [
    "### Expectations Met\n",
    "These results are much closer to what you would anticipate, despite some very strange associations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a5cfb",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "If you made it this far, thank you for your time.\n",
    "\n",
    "Feedback for this project is welcome!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
